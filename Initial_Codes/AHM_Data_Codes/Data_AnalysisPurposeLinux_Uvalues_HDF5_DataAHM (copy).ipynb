{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34d2381-a1b7-47a9-96b6-3c9416039674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pyblock2.driver.core import DMRGDriver, SymmetryTypes, MPOAlgorithmTypes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf94e558-a30e-4132-baa3-80c46b657a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_file_path():\n",
    "    current_file = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "    return current_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b929e0-9d81-489b-9805-9936b3b719c0",
   "metadata": {},
   "source": [
    "# Data Storing Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99581899-3575-4b9f-94b5-2d6c74f624fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variable_name(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a variable name dynamically based on the given parameters.\n",
    "    Parameters:\n",
    "    - params (dict): A dictionary containing 't' and 'theta_foldername' as keys.\n",
    "    Returns:\n",
    "    - str: The dynamically generated variable name.\n",
    "    \"\"\"\n",
    "    t_value = params['t']\n",
    "    u_value = params['u']\n",
    "    if t_value != 0:\n",
    "        t_formatted = \"{:.1e}\".format(t_value).replace(\"+\", \"\").replace(\"e0\", \"e\")\n",
    "    else:\n",
    "        t_formatted = \"0e0\"\n",
    "\n",
    "    if u_value != 0:\n",
    "        U_formatted = \"{:.1e}\".format(u_value).replace(\"+\", \"\").replace(\"e0\", \"e\")\n",
    "    else:\n",
    "        U_formatted = \"0e0\"\n",
    "    theta_formatted = params['theta_foldername']\n",
    "    return f\"data_t{t_formatted}_U{U_formatted}_theta{theta_formatted}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c83ba-0de0-4eca-b747-419f7c43e6b4",
   "metadata": {},
   "source": [
    "## HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65435354-ab8a-44b4-8d94-6e6950bd7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_key(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a unique key for the HDF5 dataset based on the provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - params (dict): Dictionary containing 't' and 'u' values.\n",
    "    - kwargs (dict): Dictionary of additional key-value pairs for key generation.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted unique key.\n",
    "    \"\"\"\n",
    "    t_value = params['t']\n",
    "    u_value = params['u']\n",
    "\n",
    "    if t_value != 0:\n",
    "        t_formatted = \"{:.1e}\".format(t_value).replace(\"+\", \"\").replace(\".\", \"_\").replace(\"e0\", \"e\").replace(\"-\", \"\")\n",
    "    else:\n",
    "        t_formatted = \"0e0\"\n",
    "\n",
    "    if u_value != 0:\n",
    "        U_formatted = \"{:.1e}\".format(u_value).replace(\"+\", \"\").replace(\".\", \"_\").replace(\"e0\", \"e\").replace(\"-\", \"\")\n",
    "    else:\n",
    "        U_formatted = \"0e0\"\n",
    "\n",
    "    if 'std_deviation_numberOp' in kwargs:\n",
    "        key = f\"std_deviation_numberOp_U{U_formatted}_t{t_formatted}\"\n",
    "    elif 'exp_nOp' in kwargs:\n",
    "        key = f\"expnOp_U{U_formatted}_t{t_formatted}\"\n",
    "    elif 'gndstate_energy' in kwargs:\n",
    "        key = f\"gndenergy_U{U_formatted}_t{t_formatted}\"\n",
    "    elif 'firstexcitedenergy' in kwargs:\n",
    "        key = f\"firstexcitedenergy_U{U_formatted}_t{t_formatted}\"\n",
    "    return key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff014658-8729-41b4-aaaf-edbc5c03a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_complete_path_hdf5(filename, **kwargs):\n",
    "    current_file = current_file_path()\n",
    "    if params['theta_foldername'] == '0':\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:      \n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    else:\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Define the full file path\n",
    "    file_path = base_dir / filename\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceef8db5-acaf-4ede-9f7a-d9cda91e508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders_and_store_data(params, data, filename, **kwargs):\n",
    "    \"\"\"\n",
    "    Create folders dynamically based on theta and L values, and store data in a file.\n",
    "    Writes data - overwrite if file already exists\n",
    "    Parameters:\n",
    "    - theta (float or str): The value of theta (e.g., 0, pi/4, pi/2, etc.).\n",
    "    - L (int): The integer value of L.\n",
    "    - data (DataFrame): The data to be stored.\n",
    "    - filename (str): The name of the file to store data in.\n",
    "    \"\"\"\n",
    "    # Define the base directory\n",
    "    if params['theta_foldername'] == '0':\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:      \n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    else:\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    # Create directories if they don't exist\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Define the full file path\n",
    "    file_path = base_dir / filename\n",
    "    # Writing the data to a HDF5 file in the created directory\n",
    "    data.to_hdf(file_path, key=kwargs['key'], mode='w')\n",
    "\n",
    "# filename = 'testing.csv'\n",
    "# data=  pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "# key = 'testing'\n",
    "# params['L'] = 10\n",
    "# params['theta_foldername'] = '0'\n",
    "# create_folders_and_store_data(params, data, filename, key = key, Entropy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cc9b95-d822-4459-aca6-deac9a45c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_hdf5(params, data, file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    This is useful only when the folder exists and you want to create new hdf5 file.\n",
    "    Write a single dataset to an HDF5 file under a specified key.\n",
    "    Parameters:\n",
    "    - file_name (str): The name of the HDF5 file.\n",
    "    - data (pd.DataFrame or pd.Series): The data to write to the HDF5 file.\n",
    "    - key (str): The key under which the data will be stored in the HDF5 file.\n",
    "    \"\"\"\n",
    "    data.to_hdf(file_name, key=kwargs['key'], mode='a', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58468629-3147-4f10-bc15-10f32be91b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_name_hdf5(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a file name dynamically based on the given parameters.\n",
    "    Parameters:\n",
    "    - params (dict): A dictionary containing 'L', 'nmax', and 't' as keys.\n",
    "    Returns:\n",
    "    - str: The dynamically generated file name.\n",
    "    \"\"\"\n",
    "    # Format the 't' value to scientific notation if needed\n",
    "    t_value = params['t']\n",
    "    u_value = params['u']\n",
    "    \n",
    "    if t_value != 0:\n",
    "        t_formatted = \"{:.1e}\".format(t_value).replace(\"+\", \"\").replace(\"e0\", \"e\")\n",
    "    else:\n",
    "        t_formatted = \"0e0\"\n",
    "\n",
    "    if u_value != 0:\n",
    "        U_formatted = \"{:.1e}\".format(u_value).replace(\"+\", \"\").replace(\"e0\", \"e\")\n",
    "    else:\n",
    "        U_formatted = \"0e0\"\n",
    "\n",
    "    if 'Correlation' in kwargs:\n",
    "        file_name = f\"L{params['L']}_t{t_formatted}_Nmax{params['NB_MAX']}_Nbosons{params['N_BOSON']}_U{U_formatted}_CorrelationMatrix.h5\"\n",
    "    elif 'entropy' in kwargs:\n",
    "        file_name = f\"L{params['L']}_t{t_formatted}_Nmax{params['NB_MAX']}_Nbosons{params['N_BOSON']}_U{U_formatted}_entropy.h5\"\n",
    "    elif 'expnop' in kwargs:\n",
    "        file_name = f\"L{params['L']}_t{t_formatted}_Nmax{params['NB_MAX']}_Nbosons{params['N_BOSON']}_U{U_formatted}_expnop.h5\"\n",
    "    else:\n",
    "        file_name = f\"L{params['L']}_t{t_formatted}_Nmax{params['NB_MAX']}_Nbosons{params['N_BOSON']}_U{U_formatted}.h5\"\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9f47fc-acc4-46b7-8bf8-7cc4c46558ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix_from_hdf5(filepath, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a matrix from an HDF5 file using pandas.\n",
    "    Parameters:\n",
    "    - filepath (str or Path): The path to the HDF5 file.\n",
    "    - key (str): The key to the dataset to load (default is 'correlation_matrix').\n",
    "    Returns:\n",
    "    - ndarray: The matrix loaded from the HDF5 file as a NumPy array.\n",
    "    \"\"\"\n",
    "    df = pd.read_hdf(filepath, key=kwargs['key'])\n",
    "    return df.values  # Convert the DataFrame to a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f98d899-a69e-4a68-9ff5-1286ed47e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_hdf5(params, data, filename, **kwargs):\n",
    "    \"\"\"\n",
    "    Create folders dynamically based on theta and L values, and append data to an HDF5 file.\n",
    "    \n",
    "    If the file does not exist, it will be created. If it does exist, the data will be appended.\n",
    "\n",
    "    Parameters:\n",
    "    - theta (float or str): The value of theta (e.g., 0, pi/4, pi/2, etc.).\n",
    "    - L (int): The integer value of L.\n",
    "    - data (DataFrame or Series): The data to append.\n",
    "    - filename (str): The name of the HDF5 file to store/append data.\n",
    "    - key (str): The key under which the data is stored in the HDF5 file.\n",
    "    \"\"\"\n",
    "    # Define the base directory\n",
    "    if params['theta_foldername'] == '0':\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:      \n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    else:\n",
    "        if 'Correlation' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Correl_matrix'\n",
    "        elif 'entropy' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Entropy'\n",
    "        elif 'expnop' in kwargs:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Exp_nOp'\n",
    "        else:\n",
    "            base_dir = Path.cwd() / 'AHM_Data_Codes' / f\"theta_{params['theta_foldername']}\" / f\"L{params['L']}\" / 'Remainig_Data'\n",
    "    \n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = base_dir / filename\n",
    "    # Append the data to the HDF5 file (create the file if it doesn't exist)\n",
    "    data.to_hdf(file_path, key=kwargs['key'], mode='a', index=False, append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c80589-43d7-47c2-bf93-3b44a8b41d49",
   "metadata": {},
   "source": [
    "# Data for a $\\theta$, $L$, $N\\_{Boson}$, $N\\_{Max}$, $U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ddfdc9-0ba0-4c7a-a2ef-936f5ae00057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params={}\n",
    "params['L'], params['N_BOSON'] = 40, 40\n",
    "params['theta_foldername_list'] = ['0', 'Piby4', 'Piby2', '3Piby4', 'Pi']\n",
    "params['theta'], params['theta_foldername'] = np.pi, params['theta_foldername_list'][4]\n",
    "params['t'] = 1\n",
    "params['u'] = 1\n",
    "params['mu'] = 0\n",
    "params['NB_MAX'] = 3 # max n_boson per site\n",
    "params['theta_list'] = [0, np.pi/4, 3*np.pi/4, np.pi/2, np.pi]\n",
    "params['t_list'] = np.arange(0.01, 2, 0.1)\n",
    "\n",
    "driver = DMRGDriver(scratch=\"./tmp\", symm_type=SymmetryTypes.SAny|SymmetryTypes.CPX, n_threads=4)\n",
    "\n",
    "driver.set_symmetry_groups(\"U1\")\n",
    "Q = driver.bw.SX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d440c38a-272f-4fa6-823f-c14a77b7e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_basis, site_ops = [], []\n",
    "for k in range(params['L']):\n",
    "    basis = [(Q(i), 1) for i in range(params['NB_MAX'] + 1)] \n",
    "    ops = {\n",
    "        \"\": np.identity(params['NB_MAX'] + 1),                           # identity\n",
    "        \"C\": np.diag(np.sqrt(np.arange(1, params['NB_MAX'] + 1)), k=-1), # b+\n",
    "        \"D\": np.diag(np.sqrt(np.arange(1, params['NB_MAX'] + 1)), k=1),  # b\n",
    "        \"N\": np.diag(np.arange(0, params['NB_MAX'] + 1), k=0),           # particle number\n",
    "        \"A\": np.diag(np.sqrt(np.arange(1, params['NB_MAX'] + 1))*np.exp(1j * params['theta'] * np.arange(params['NB_MAX'])), k=-1), # A+_withPhase  \n",
    "        \"B\": np.diag(np.sqrt(np.arange(1, params['NB_MAX'] + 1))*np.exp(-1j * params['theta'] * np.arange(params['NB_MAX'])), k=1), # A_withPhase  \n",
    "    }\n",
    "    site_basis.append(basis)\n",
    "    site_ops.append(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60024b09-ef05-4c8d-a818-a606d1dcb292",
   "metadata": {},
   "source": [
    "## First Excited Gap, Correlation, Number Operator and Entropy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5e942-8643-4cb1-a358-3fb2e9ff8856",
   "metadata": {},
   "source": [
    "#### $U = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "648a6640-041e-406c-9da2-691d5aab9003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sweep =    0 | Direction =  forward | Bond dimension =   50 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =      0.464 | E[  2] =     -46.1685991489    -45.9957023194 | DW = 3.46402e-05\n",
      "\n",
      "Sweep =    1 | Direction = backward | Bond dimension =   50 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =      2.342 | E[  2] =     -47.9846711845    -47.8661407465 | DE = -1.87e+00 | DW = 8.37581e-05\n",
      "\n",
      "Sweep =    2 | Direction =  forward | Bond dimension =   50 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =      5.102 | E[  2] =     -48.5845653584    -48.5295329688 | DE = -6.63e-01 | DW = 4.68341e-05\n",
      "\n",
      "Sweep =    3 | Direction = backward | Bond dimension =   50 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =      7.160 | E[  2] =     -49.3390908592    -49.3200920480 | DE = -7.91e-01 | DW = 1.35182e-04\n",
      "\n",
      "Sweep =    4 | Direction =  forward | Bond dimension =  100 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     12.628 | E[  2] =     -50.0101453147    -49.9749494572 | DE = -6.55e-01 | DW = 1.68193e-05\n",
      "\n",
      "Sweep =    5 | Direction = backward | Bond dimension =  100 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     20.974 | E[  2] =     -50.8211429335    -50.7367127483 | DE = -7.62e-01 | DW = 6.08316e-05\n",
      "\n",
      "Sweep =    6 | Direction =  forward | Bond dimension =  100 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     32.343 | E[  2] =     -51.3289711742    -51.2774712234 | DE = -5.41e-01 | DW = 6.90525e-05\n",
      "\n",
      "Sweep =    7 | Direction = backward | Bond dimension =  100 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     41.123 | E[  2] =     -51.6661527410    -51.6083373724 | DE = -3.31e-01 | DW = 6.19980e-05\n",
      "\n",
      "Sweep =    8 | Direction =  forward | Bond dimension =  150 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     60.007 | E[  2] =     -51.9640840335    -51.9039783911 | DE = -2.96e-01 | DW = 2.89553e-05\n",
      "\n",
      "Sweep =    9 | Direction = backward | Bond dimension =  150 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =     83.601 | E[  2] =     -52.1712616997    -52.1161730344 | DE = -2.12e-01 | DW = 4.00058e-05\n",
      "\n",
      "Sweep =   10 | Direction =  forward | Bond dimension =  150 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    112.794 | E[  2] =     -52.3130890708    -52.2499351485 | DE = -1.34e-01 | DW = 3.83987e-05\n",
      "\n",
      "Sweep =   11 | Direction = backward | Bond dimension =  150 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    138.152 | E[  2] =     -52.4089746229    -52.3356367034 | DE = -8.57e-02 | DW = 4.50749e-05\n",
      "\n",
      "Sweep =   12 | Direction =  forward | Bond dimension =  200 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    176.637 | E[  2] =     -52.4578392421    -52.3985306473 | DE = -6.29e-02 | DW = 1.32148e-05\n",
      "\n",
      "Sweep =   13 | Direction = backward | Bond dimension =  200 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    226.025 | E[  2] =     -52.4765545352    -52.4243594637 | DE = -2.58e-02 | DW = 1.81604e-05\n",
      "\n",
      "Sweep =   14 | Direction =  forward | Bond dimension =  200 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    281.229 | E[  2] =     -52.4803901719    -52.4376028015 | DE = -1.32e-02 | DW = 2.84511e-05\n",
      "\n",
      "Sweep =   15 | Direction = backward | Bond dimension =  200 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    331.814 | E[  2] =     -52.4806623393    -52.4385560006 | DE = -9.53e-04 | DW = 2.93663e-05\n",
      "\n",
      "Sweep =   16 | Direction =  forward | Bond dimension =  250 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    397.871 | E[  2] =     -52.4819603326    -52.4393008841 | DE = -7.45e-04 | DW = 9.65702e-06\n",
      "\n",
      "Sweep =   17 | Direction = backward | Bond dimension =  250 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    481.345 | E[  2] =     -52.4821613721    -52.4397933498 | DE = -4.92e-04 | DW = 8.81575e-06\n",
      "\n",
      "Sweep =   18 | Direction =  forward | Bond dimension =  250 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    565.110 | E[  2] =     -52.4821435543    -52.4400817498 | DE = -2.88e-04 | DW = 9.32452e-06\n",
      "\n",
      "Sweep =   19 | Direction = backward | Bond dimension =  250 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    646.469 | E[  2] =     -52.4821452871    -52.4401066508 | DE = -2.49e-05 | DW = 8.21439e-06\n",
      "\n",
      "Sweep =   20 | Direction =  forward | Bond dimension =  300 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    748.431 | E[  2] =     -52.4826682825    -52.4403273214 | DE = -2.21e-04 | DW = 4.77594e-06\n",
      "\n",
      "Sweep =   21 | Direction = backward | Bond dimension =  300 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =    878.496 | E[  2] =     -52.4827346073    -52.4406649467 | DE = -3.38e-04 | DW = 4.97246e-06\n",
      "\n",
      "Sweep =   22 | Direction =  forward | Bond dimension =  300 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1004.834 | E[  2] =     -52.4827375556    -52.4409305062 | DE = -2.66e-04 | DW = 5.25413e-06\n",
      "\n",
      "Sweep =   23 | Direction = backward | Bond dimension =  300 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1135.106 | E[  2] =     -52.4826988729    -52.4409208089 | DE = 9.70e-06 | DW = 4.49794e-06\n",
      "\n",
      "Sweep =   24 | Direction =  forward | Bond dimension =  350 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1294.376 | E[  2] =     -52.4829573679    -52.4410825110 | DE = -1.62e-04 | DW = 2.99823e-06\n",
      "\n",
      "Sweep =   25 | Direction = backward | Bond dimension =  350 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1461.064 | E[  2] =     -52.4829436710    -52.4415880429 | DE = -5.06e-04 | DW = 3.39286e-06\n",
      "\n",
      "Sweep =   26 | Direction =  forward | Bond dimension =  350 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1617.918 | E[  2] =     -52.4829489822    -52.4419586439 | DE = -3.71e-04 | DW = 3.41020e-06\n",
      "\n",
      "Sweep =   27 | Direction = backward | Bond dimension =  350 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1781.661 | E[  2] =     -52.4829709461    -52.4420542672 | DE = -9.56e-05 | DW = 3.11363e-06\n",
      "\n",
      "Sweep =   28 | Direction =  forward | Bond dimension =  400 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   1990.236 | E[  2] =     -52.4830906838    -52.4423413512 | DE = -2.87e-04 | DW = 2.03937e-06\n",
      "\n",
      "Sweep =   29 | Direction = backward | Bond dimension =  400 | Noise =  1.00e-04 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   2244.808 | E[  2] =     -52.4830891342    -52.4425830450 | DE = -2.42e-04 | DW = 2.29831e-06\n",
      "\n",
      "Sweep =   30 | Direction =  forward | Bond dimension =  400 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   2501.698 | E[  2] =     -52.4830885057    -52.4427169452 | DE = -1.34e-04 | DW = 2.14784e-06\n",
      "\n",
      "Sweep =   31 | Direction = backward | Bond dimension =  400 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   2770.828 | E[  2] =     -52.4831036932    -52.4431108525 | DE = -3.94e-04 | DW = 1.99073e-06\n",
      "\n",
      "Sweep =   32 | Direction =  forward | Bond dimension =  450 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   3107.140 | E[  2] =     -52.4831718136    -52.4436796962 | DE = -5.69e-04 | DW = 1.46916e-06\n",
      "\n",
      "Sweep =   33 | Direction = backward | Bond dimension =  450 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   3493.078 | E[  2] =     -52.4831695843    -52.4439950245 | DE = -3.15e-04 | DW = 1.60617e-06\n",
      "\n",
      "Sweep =   34 | Direction =  forward | Bond dimension =  450 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   3882.501 | E[  2] =     -52.4831778142    -52.4443007046 | DE = -3.06e-04 | DW = 1.48413e-06\n",
      "\n",
      "Sweep =   35 | Direction = backward | Bond dimension =  450 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   4256.808 | E[  2] =     -52.4831883029    -52.4445934188 | DE = -2.93e-04 | DW = 1.37338e-06\n",
      "\n",
      "Sweep =   36 | Direction =  forward | Bond dimension =  500 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   4712.205 | E[  2] =     -52.4832309411    -52.4448403056 | DE = -2.47e-04 | DW = 1.04199e-06\n",
      "\n",
      "Sweep =   37 | Direction = backward | Bond dimension =  500 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   5242.876 | E[  2] =     -52.4832325630    -52.4451925158 | DE = -3.52e-04 | DW = 1.12094e-06\n",
      "\n",
      "Sweep =   38 | Direction =  forward | Bond dimension =  500 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   5747.172 | E[  2] =     -52.4832388068    -52.4455675829 | DE = -3.75e-04 | DW = 1.02503e-06\n",
      "\n",
      "Sweep =   39 | Direction = backward | Bond dimension =  500 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   6184.705 | E[  2] =     -52.4832458599    -52.4457285032 | DE = -1.61e-04 | DW = 9.39316e-07\n",
      "\n",
      "Sweep =   40 | Direction =  forward | Bond dimension =  550 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   6735.742 | E[  2] =     -52.4832746365    -52.4459742834 | DE = -2.46e-04 | DW = 7.31288e-07\n",
      "\n",
      "Sweep =   41 | Direction = backward | Bond dimension =  550 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   7373.596 | E[  2] =     -52.4832746752    -52.4464012159 | DE = -4.27e-04 | DW = 8.01536e-07\n",
      "\n",
      "Sweep =   42 | Direction =  forward | Bond dimension =  550 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   8029.722 | E[  2] =     -52.4832761494    -52.4465875077 | DE = -1.86e-04 | DW = 7.57400e-07\n",
      "\n",
      "Sweep =   43 | Direction = backward | Bond dimension =  550 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   8648.311 | E[  2] =     -52.4832830520    -52.4467338216 | DE = -1.46e-04 | DW = 6.74112e-07\n",
      "\n",
      "Sweep =   44 | Direction =  forward | Bond dimension =  600 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =   9293.848 | E[  2] =     -52.4833029551    -52.4470149546 | DE = -2.81e-04 | DW = 5.23806e-07\n",
      "\n",
      "Sweep =   45 | Direction = backward | Bond dimension =  600 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  10079.454 | E[  2] =     -52.4833037630    -52.4472653996 | DE = -2.50e-04 | DW = 5.43576e-07\n",
      "\n",
      "Sweep =   46 | Direction =  forward | Bond dimension =  600 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  10818.170 | E[  2] =     -52.4833040028    -52.4474981459 | DE = -2.33e-04 | DW = 5.32262e-07\n",
      "\n",
      "Sweep =   47 | Direction = backward | Bond dimension =  600 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  11612.304 | E[  2] =     -52.4833087961    -52.4477241328 | DE = -2.26e-04 | DW = 4.83957e-07\n",
      "\n",
      "Sweep =   48 | Direction =  forward | Bond dimension =  650 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  12448.524 | E[  2] =     -52.4833225373    -52.4479506751 | DE = -2.27e-04 | DW = 3.74730e-07\n",
      "\n",
      "Sweep =   49 | Direction = backward | Bond dimension =  650 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  13397.444 | E[  2] =     -52.4833221439    -52.4482080238 | DE = -2.57e-04 | DW = 3.95021e-07\n",
      "\n",
      "Sweep =   50 | Direction =  forward | Bond dimension =  650 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  14339.186 | E[  2] =     -52.4833226927    -52.4483781262 | DE = -1.70e-04 | DW = 3.86324e-07\n",
      "\n",
      "Sweep =   51 | Direction = backward | Bond dimension =  650 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  15271.479 | E[  2] =     -52.4833247705    -52.4484702114 | DE = -9.21e-05 | DW = 3.56368e-07\n",
      "\n",
      "Sweep =   52 | Direction =  forward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  16205.376 | E[  2] =     -52.4833346901    -52.4485724089 | DE = -1.02e-04 | DW = 2.79159e-07\n",
      "\n",
      "Sweep =   53 | Direction = backward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  17296.504 | E[  2] =     -52.4833344521    -52.4488044427 | DE = -2.32e-04 | DW = 3.05201e-07\n",
      "\n",
      "Sweep =   54 | Direction =  forward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  18375.659 | E[  2] =     -52.4833354924    -52.4489759966 | DE = -1.72e-04 | DW = 2.87002e-07\n",
      "\n",
      "Sweep =   55 | Direction = backward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  19415.081 | E[  2] =     -52.4833359940    -52.4490833286 | DE = -1.07e-04 | DW = 2.75889e-07\n",
      "\n",
      "Sweep =   56 | Direction =  forward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  20486.722 | E[  2] =     -52.4833369231    -52.4491496373 | DE = -6.63e-05 | DW = 2.61094e-07\n",
      "\n",
      "Sweep =   57 | Direction = backward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  21555.268 | E[  2] =     -52.4833376074    -52.4491770545 | DE = -2.74e-05 | DW = 2.52129e-07\n",
      "\n",
      "Sweep =   58 | Direction =  forward | Bond dimension =  700 | Noise =  1.00e-05 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  22547.764 | E[  2] =     -52.4833381373    -52.4492079670 | DE = -3.09e-05 | DW = 2.47314e-07\n",
      "\n",
      "Sweep =   59 | Direction = backward | Bond dimension =  700 | Noise =  0.00e+00 | Dav threshold =  1.00e-10\n",
      "Time elapsed =  23609.635 | E[  2] =     -52.4833384180    -52.4492372744 | DE = -2.93e-05 | DW = 2.37582e-07\n",
      "\n",
      "ATTENTION: DMRG is not converged to desired tolerance of 1.00000e-08\n",
      "Execution time: 23609.71 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=60, bond_dims=[50] * 4 + [100] * 4 + [150] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4 + [650] * 4 + [700] * 4,\n",
    "    noises=[1e-4] * 30 + [1e-5] * 29 + [0], thrds=[1e-10] * 60, dav_max_iter=30, iprint=1)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a84968-99cc-42b5-bcd6-5dcd4a8f1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c637f19-40c9-4ba0-b238-6170ad8ae4e5",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a33a6a6-6bac-4674-b58a-3cfbdd0e8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad2e00f-857c-443b-a694-647b2fffb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002624015771146903\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562802b1-f737-4a60-a26d-afdaa560504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.00023694153658134542 --> 600, ket -- 50, sweeps 10\n",
    "0.00019492616866190876 \n",
    "0.0003254248894209413 --> 650, ket -- 50, sweeps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac792a21-27b8-45e0-abb9-31fc11bea29c",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "13a39a34-82de-4849-ad40-3aea6222cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80464d04-1bcf-4a88-8f00-2092f8b5dba5",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c5b56efc-4e4e-404c-9c88-cdd7ad7c4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b37590-6058-4e0d-8bc0-0e0d00804d84",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c5fc4190-cd87-4cb7-a244-ee4d9f6777d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "654ab3d5-b19b-4095-81fe-2f07631527ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751ad41-b149-4c66-8120-629fe1ad0695",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "04c0ecd5-fd2e-419a-b9ba-cd25c57c8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5c4c7-65e3-4f2d-86a3-fec677aa4d65",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bac5cdf3-62dd-4883-9407-ff7bafb6bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "aee2f440-2765-4244-b942-016a945907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad71df-ac81-4cb8-a775-a507483392c1",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3f02995b-0520-4ce3-8cf6-c0381589afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fe52261d-22b7-48e0-832f-8f2d296ad47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6289591764258072+3.323139823956348e-20j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "3d54a01e-fb87-4275-bbb1-faed51e2a9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U0e0_t1_0e0\n",
      "/std_deviation_numberOp_U0e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0675d9a-17c3-4c38-82e3-00a810f24ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666d4c0-6b15-49bf-bffd-6ddd967b653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865267f-0989-480d-8101-aceb14489eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['t'] = 0\n",
    "update_csv_column(\"theta0_AHMDmrg_data.csv\", 'Entropy', 't', params['t'], str(entropy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad76b7-7007-4ca8-aebc-11b45a215d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kets[0])\n",
    "print(entropy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff46c6-2cc1-45ab-974d-67873c93e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy2 = driver.get_bipartite_entanglement(kets[1])\n",
    "print(entropy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429b49d-8936-4199-976e-0da566a9be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"theta0_AHMDmrg_data.csv\")\n",
    "params['t'] = 0\n",
    "condition = df['t'] == params['t']\n",
    "df.loc[condition, 'Entropy'] = str(entropy1) * condition.sum()\n",
    "\n",
    "# Step 4: Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(\"theta0_AHMDmrg_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0dc37-296a-41c8-be5a-3a968770117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d346695-f3e3-40c1-8afb-9ccf73016a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stringlist_to_float_list(string):\n",
    "    # Remove the square brackets and split by spaces\n",
    "    float_list = list(map(float, string.strip('[]').split()))\n",
    "    return float_list\n",
    "\n",
    "def get_data_by_conditions(df, conditions, columns):\n",
    "    condition = pd.Series([True] * len(df)) \n",
    "    for key, value in conditions.items():\n",
    "        condition &= df[key] == value\n",
    "    filtered_df = df[condition]\n",
    "    return filtered_df[columns] if not filtered_df.empty else None\n",
    "colors1 = plt.cm.Reds(np.linspace(0.5, 0.95, len(params['theta_list'])))\n",
    "conditions = {'t': 0} \n",
    "keywords = ['Entropy']\n",
    "result = get_data_by_conditions(df, conditions, keywords)\n",
    "entropy = convert_stringlist_to_float_list(result.values[0][0])\n",
    "print(entropy)\n",
    "plt.plot(list(range(len(entropy))), entropy, \"--o\", label=f'$θ$={params['theta_list'][0]:.2f}', color = colors1[0], ms=8)\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.xlabel('Different Combinations of Partition of two subsystems')\n",
    "plt.ylabel('Entropy')\n",
    "U, t = params['u'], 0\n",
    "plt.title(f'Entanglement Scaling Laws for J = {t}, U={U}')\n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a026de-8fe0-4c92-9d43-ebab802b9132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbbc1d46-fb15-45a2-87e7-651aa2bc8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 149.20 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.1\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd2396cb-89fb-4d58-a529-cd341f3d2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415b556-dda9-4316-be76-7291c2f0e405",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4b28403-336a-4fea-b64b-04384e17081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e53fbe24-d824-45b3-9ca4-fb9562c1c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015714134280301892\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e28eb6-92ed-4e95-9e7c-d30a69952aa7",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53a7f1f7-a2a7-4535-b1f6-6fa180f01ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575c8d4-45cf-4e4a-bc39-440d3fc4be0e",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2e87e59-8d40-4658-951d-a7fe57ad6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename1, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ffe55-b343-4a42-9802-d87a2a83cf32",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d10d74d1-7b0b-4f24-b75e-757a33f9190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f060ddef-cd68-420f-bb3f-c1469a370f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab1a6b-888f-4299-9b75-befbb7dca84d",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfaf28cb-21cc-411b-bede-d582dc1fa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f2472-4297-4975-948e-e0a78f4bdc0d",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f36a3c8f-3010-4434-a2c0-5b651c194924",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename1, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6fcab3e-34b0-4b2b-8e16-c96e90f9c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4cbcd-8f2c-4d78-9412-77e941704a91",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "803e326b-d585-47b1-ac5c-c6e545444818",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faa58bc8-d10b-4dfd-b511-036129f66799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.794926377208388-8.7020146339946e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00ea986f-ca13-4365-a0c5-8d3789fb9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U1_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057d1e1-c1e4-41b0-a285-a7a1761484f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5b1efe7-fd37-4533-916c-04882b808a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 142.58 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.2\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05c44ad2-3ba5-4a02-9ff9-a18f8a397b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0763ec-8488-4b55-a10a-3582778478dd",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5ef0af2-41ea-4314-9f07-06bc51c3a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c9bc344-24ce-4997-97f8-f4a8c3795da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010880418521860727\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df8027-f55e-4a95-a20d-82fd082f2422",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2be249b9-520f-4a35-a10c-8484eed7ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52e0d2-f587-4a66-9838-4a1e587abe2c",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ca44885-efa2-486d-a8f0-17ab876bf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154abbf-0302-413d-9c28-a2bbddfedfcd",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b92057f-646b-4630-a4b0-8870fa1e7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b9728ed-5116-47cc-95e7-9fb4a60f838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738bb96-ecd0-4444-9d1b-6ff57c610ae5",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f346c7b-b8e9-4f81-beb4-178df21aded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d7b01-5a16-403a-a680-b4c8e63877ad",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd4bd93d-858b-4bfb-b092-0dcfc394a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9dfa3c3-ded7-45ce-b833-a6a301e85bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85641400-71b1-4350-9e5f-5dfd6dbe6b34",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "468ba8f9-8684-4ad4-a9d8-3bf060066442",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c2a54cd-ef9c-43a5-a98b-ad1df532cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7635076892736257+3.1416591148420717e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d2a86b9-6e51-46e1-ae22-ba5419be287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U2_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U2_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6400934-96b5-45c4-9070-d83203a900d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8f2ba00-150a-41f8-a3df-2c2a0baa0a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 143.02 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.3\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e973de4d-1c47-43c2-9e13-a582a61d8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ddb137-857c-4d00-b760-501d2ba80178",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac1ddf0d-3e29-40fc-9328-76dd017873c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "651e8ccf-1cf7-448b-a27a-7bda027edfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011539117091653305\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4636446-887d-46e5-bf92-d2d3061804b8",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f26fe263-79a8-4d9e-81c5-e8e02344b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6c6fa-b0f7-4d8c-b51e-048a4c2db381",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4f49d1a-8172-4c4d-8eb2-5e09e2fb2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a348611-16b8-4204-a641-68bf909eac61",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6c1dd3b-09b6-4fbe-bb43-cc7f10933c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22683846-308b-456f-a606-11ec41468a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa6d5d-b666-4b06-bf1a-deeda42ce04c",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60834e54-8927-4c58-a05e-15202e6aa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a50dc-ccf6-4fbe-b560-c079b5634b79",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7a75166-ae38-4994-ba82-8340927ae791",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d56ce2aa-9e8c-4c21-93a6-e922817cedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4994f-60b4-4768-8151-448d2dbd4fcd",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d3d8c5a-28d6-4171-96a1-45f07d75be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c87a8134-1d29-45b6-a3f9-187bbd4351f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7039209107450427-7.547488197791026e-21j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c62bb79-d047-4a93-8c93-ac581f9712fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U3_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U3_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced1257-c4fd-4e99-a428-e3ce25780a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06aef3e4-7e9e-4ff3-836a-b38278d5006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 149.25 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.4\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6935d52-2b3d-4d9b-ae97-07cfef1ae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64cf02-da28-4fb6-aa48-4231f4081047",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36012d14-6f7d-4ffd-83b5-61d1f791f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6146da1-c570-4fd4-91ac-cb0cb76f7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015630026808742465\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31484e3-b265-45d4-8e91-cf63b829c1c7",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc0b8660-18a3-4514-88ec-d02a77641edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09d405-1919-4009-ad88-3d1841d917a3",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d6f6724-0f1f-48cd-8fe6-b51bf79e977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e48f7-1c71-4814-b274-dfee2e231850",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b2d249d-a993-4eb6-a772-10f57b58a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "862dd203-0c85-4285-bd8d-691221bd218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d95ce-b8f7-4980-876e-13d620b65bf1",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6246c6ee-2951-45d2-b2d0-9535d242303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0ddf5-d559-4a9a-898f-2c0ab9178035",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bfa59c0-223c-48f8-ac60-9801da938048",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f4c495f-c0c4-43f4-831d-949c6d73c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b46e15-3f09-499c-8d3b-21a0e2481892",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3946b1e6-dc26-4c3b-b764-8827113aa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c91b5d43-6858-4bac-8292-12f18d8c5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6707181475698424+6.269397721475861e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c554bffa-d487-451f-89c3-b93b31d8a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U4_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U4_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae700bef-3d65-401c-b73a-f9924694db79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82296a7d-d5f8-4d3b-af29-e894e0ac4e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 137.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.5\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38ab9729-07b5-421c-ad9f-ba7eb4713223",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5e88e-cd42-40b3-94bf-f8dce750fd96",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c3936b6-f64c-44c3-9723-de6dbb76432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c36e73b1-d539-4a6b-8719-942c3f62b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021403881464154458\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9a0ab-f66f-4b8c-8a68-7fc84aca5c32",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8f5f07f1-344e-43f2-bff9-d4832fc86afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561345c0-dfd0-44ef-bd8d-ef8c78830acf",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "912bf400-bd6d-47ae-ad06-6eb114fbaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb23e74-714a-44de-9274-23a62b3926fa",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2088a7dc-05cf-40f1-8c3c-245aaa2b46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e77b1323-c7fa-4071-9ede-989e378c8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e1d55-3a15-4fc5-9ecd-766ea2384872",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d306f66-6c7b-4876-b580-25452e214e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2fc06-d8a6-4643-9cc8-66455dee63ef",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd2b3caa-9237-4ba9-b960-94eb3c6567b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "13ff5d5e-d31a-4d47-ad39-c99c5044e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759bc1-1a13-4040-b243-3a851d1a6587",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "313e311c-570d-487b-82d2-08d7f602bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0ef511f5-8dba-4520-bddd-4bb93fe7bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.658768254967465-5.932890704418073e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bc86a6d5-f40a-4929-9961-9cb61e26427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U5_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U5_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ea26b-552f-4171-b99f-a9a665be5bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2bdb4cd1-c731-4cab-9769-d6206a028958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 150.40 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.6\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "79c03cd7-c31a-4a91-b1a5-33d378d11065",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cc4db-9f3a-4938-a592-1cc4ed9e6a3a",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2830f13b-c943-4efe-acd9-bed280ebcb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1a88aec3-4a66-43b8-9195-d6246c892e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004685894556920996\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfcbe3-bfdd-4112-b900-df2c4c481d62",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0b503df-675d-4d59-bf70-f1c3cca7173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b6bbf-a07c-46c4-935e-395cee1732c9",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27e83122-91e4-4010-9b16-9a86bf99727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa4943-d22a-46fc-aa21-0a815f581464",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4e8e219-5fe1-44e1-88ff-c954f6b5054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b7aef4b6-3c3b-4b44-abaf-b8d4bd879e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9846c-9888-4752-bdc7-79027ffd77ec",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9004b2e-1cee-4e44-96e6-26b571e04a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d22434-bdab-4f68-9b60-b723a630aa36",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "78c276fd-3ba9-4554-8acd-89c6a2a75f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "74666b4f-001c-463c-814a-2343f4fbab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69803b1-99d4-4ce6-8e7f-ce7955d33903",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "65b90427-3ffb-43c7-beab-d866b9af675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b4b7351-ce6c-4d0a-9c30-6f7876757cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.659217576605545+7.415447086929104e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b48c9c42-71b6-4c66-a8f4-9d330ff7f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U6_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U6_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0e445-1c44-4baf-9f10-d754e672268e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aed2f58d-8ad8-4e6b-bd08-eef4ffec43ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 147.89 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.7\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a1e8fc7-ea5d-4902-bfcf-9fede327edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893334a-2df0-41a4-a847-ee5875080835",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5895b671-2db5-41ff-9f1c-9703684b7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "48a8301c-fe52-43f8-ac29-85f99a8f4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022178302018838318\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abc494-451c-49e4-a3c3-c2b4003ce0ac",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf00ccea-e398-483b-b82e-e5a337724e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cdaaa-5fcc-4fe9-be6c-f2283d913319",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e5a1d25e-7bb8-42e9-a6e5-4b5a47853370",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a61809-f7af-4c40-a5f1-86dae6352f9c",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0bd454d4-0ae0-4d15-b7e6-ec78ac825786",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26be18e9-21c1-4b06-9ac5-7c20f08f6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce973a9e-cbb1-4a5f-81be-35101961d0bd",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "635beafe-0610-4acb-be1d-d5972d780fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980b5a3-e794-4257-8a5e-b2cfede6a996",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3695a8cb-fa8a-426f-85b9-2a2a73068e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e6cffe4-cad1-45d0-a0b5-70a6af7f2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f1bf8-06c4-43d8-9e85-a7f006c18719",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddfb2280-0d11-4879-8165-baeafef1fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bf4202a8-ea24-4bdc-99f9-d8cb6bf395c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6429408817001884+1.9747708314194016e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e5184b76-85df-4f8c-826e-a18630956d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U7_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U7_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1120e-6dd8-4380-af7e-55ee8e8687a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "14b671fc-d658-4bf9-9b75-924c0329bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 158.31 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.8\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4  + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cd16794b-fc0f-4561-b655-108416e7dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392b38a-2317-4b52-932a-c8fa80fb6631",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "25b490f1-4e66-4f48-9170-3c03d85c637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0e516bd0-6f46-4906-bdba-c7b3625a8e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006874561767174338\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901d5a5-bb64-4ba4-ad32-065b0f32c4dd",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fb27cf4f-e27a-4916-a443-4cb60698e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2cc0d-85ab-4a4a-9651-2eeb66f5c19a",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "feb21e56-b40d-41cd-bd26-5e1985bfd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a4b6a-91e6-41f4-9a19-9ca8acc48e6f",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dacacfa8-25ad-4a66-ae7a-04aa081255f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3478835f-916e-4e28-9143-636e82d397b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f518d4-a694-44a1-8e97-43f20b1efdc5",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "717d1b34-fe95-460e-8bf9-e2cd9c716fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5bcdd-506a-4541-89c4-c48c6397944e",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d3c30c45-11d4-4d3a-a911-beec5c5940eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8adeaa4b-0d38-4ec2-a198-2b525030f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047f11e-ef66-4e6c-81b2-a1ba8ddf727e",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d65600f2-6253-4026-9a3c-44f2f9a13672",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1d6f031d-e06d-4934-95a6-eb6a3ffb7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6384506898164473-8.47959028481768e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5315e292-ea07-4356-8878-585eebbefb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U8_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U8_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a1a0b-d5d6-4850-87fc-43b490961d5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "def54c55-3a50-44af-b4d9-e4592d831138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 149.28 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 0.9\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0287e058-2a03-4d9a-96fb-35f482d31e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3d067-cb91-485d-aaaf-e628e8ff09ea",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "356b6f80-dce6-4536-b1df-63bc2683548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "48929afc-8dd1-40ac-a8c5-435cdea49cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001181550171635085\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb63787-2c8f-4dfc-9f41-4d3781a35207",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "567877a6-3395-48b2-9db7-a99dabaec59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfd0a5-320f-4d64-9f2d-fb08065239d4",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cf427d6d-c64d-4f67-a577-bf769fbe8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955bbee-2b30-46c2-80b4-3d065d3c9ca4",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5a66f2e7-9fb7-4b4e-aee9-5a12ddba43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "468a6f20-f5cf-4af0-80d7-6b1fe0684d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443ce51-d1fd-4fb9-a676-bed77b206bec",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "37389791-8f85-45ea-9630-63a3efc22747",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c81d2-ad29-4461-a59b-f024a14143ec",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6faa979a-f37f-4670-bd5b-59fdd662b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fb98278f-0640-4c0a-9343-fdbdb14790cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8287216-b26b-4684-845f-1a2e10a60fc3",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e96c6117-5201-49cb-9803-fb39f9ffad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2bc2767d-154f-4617-a8b0-24c6cc6ac0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6304910276293414+1.7628975048141727e-17j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ca514918-3252-4431-9f4c-60457ac6ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U9_0e01_t1_0e0\n",
      "/std_deviation_numberOp_U9_0e01_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065e2b3-5ac5-48ef-b5ae-a5600a9cb429",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "778e3828-e1f4-4cb5-b6e1-df07061a3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 148.51 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d4c419ff-5df2-48bd-bf78-4b52259447a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902e024-54dc-4d14-9616-7ee62e4ca324",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "91e8176e-b2da-4f00-86fa-90b7b1e561fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "07cef1aa-ba07-4835-894d-91e691eaa1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018682493306533265\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492b109-e57d-4752-901c-b8d14bf5d3e4",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "83eda10e-a6fd-48d1-b370-3ccce8445501",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb43e7-e940-41f0-9909-8f456c45a1cf",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6c704284-57c7-499c-8b7a-66058e246395",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2800d-e9d7-492a-89e1-261c8b61aef2",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fa5af5f9-35cd-4c31-81aa-1e6a23f7365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a203ed0c-7f68-4f72-8020-5f82cf138d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76042f-379b-4065-97db-8fcae397b219",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ef53fdd5-65f2-4b93-a530-da43a4eef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e0590-42dd-434e-853d-635a05697c72",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2bd1e1f5-869c-423d-8c90-1dd441593c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1a04dbe6-9eb0-4065-8317-7c7039630686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67dde5a-e3f4-4219-8c48-d83cd4dee277",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3dfc1c9f-235b-4ce9-a7f3-b0904ea9add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "db256870-ba39-48fe-a01d-2af4e6e473f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6173579594599204-2.3371514696518006e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "85f16a8b-b813-4e65-864c-e1126ac67591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_0e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_0e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450d4db-2687-4e3f-b971-767f445ad30b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0864a0d3-2aad-4711-b688-8664a5953e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 150.56 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.1\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1ad6ac8b-a3bf-4af4-bee5-af985d26d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c636c1f-1129-4605-96ef-9043858735fa",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0cd7e01b-ed79-44e4-a860-15e6e37878e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "150cf460-5046-4b67-a5d3-2dde0f2d2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009609495409292019\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0985f-9470-4494-8031-291b625a3a0f",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bf9f8ed4-df5e-414e-a572-eb532df1841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639d006-80f4-4380-9721-82d3124b1fbd",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e2fd6791-a95c-4d02-905f-3a6db0520977",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "create_folders_and_store_data(params, entropy, filename1, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170f224-eabf-44a9-8059-a771bba7aa7e",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "78534b25-1b7d-4b3a-b850-a7b4cde0f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4f043830-9f7b-4fcb-b313-98579b2bbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b60951-dfed-47f8-a133-d6e1b7867ee0",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c5564fe4-738a-4a93-ad2f-5705f171dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efde776-3af8-47de-8f9e-d490ac6ed9e6",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8a091bed-c3e7-476b-aa7f-25e504201830",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ade4f928-fcec-47fe-ba71-cf963660a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a18293-cceb-462d-b9b4-ce658db45f2e",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "dd40a1ec-dea1-43f5-a785-b9156ca40618",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a915d152-05e5-4cb2-908f-50de59b93b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.611390057297839-3.111390381940938e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c2aed4dd-1418-4e6e-ba24-e14ca276e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_1e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_1e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07748ba-c2a2-4047-bab5-30ca66bec080",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b9fcb798-66d3-4c7a-a8a4-598236fce087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 154.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.2\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5c5e526d-96aa-4d6a-86d1-daaefec9089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7162c8-82a6-4c0b-9524-8ba839082852",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dc42e1eb-e159-42c0-914d-4885aab075ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3d144a6c-1367-4304-85cf-377fae08ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000871812134525353\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc24e-98a9-4381-af17-f4b4ef605cff",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f56e258d-9cc5-40f7-9c31-d3616cd58296",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b26c08-29ae-4fb7-b606-103ed15412c2",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0bd8822a-fd22-4c6c-b66b-2de92815a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5aedc1-c08a-41e2-9447-035ba94b6f6c",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "304d5dbd-d605-433c-be6f-edbd5f5e9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0e3f6385-289f-47f3-9cce-621dfe2d17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc7d3c-0f7b-41df-8f2f-c44eeb48f73a",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7afaa8a7-2b45-4a89-8eaa-6a796c63e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a11ce3-dfe1-4dc0-b064-7aeb1549cb28",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "81c9959d-d064-43ae-b9fb-21f92783a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b0938d3a-1b9c-46aa-a4f5-6090e60c3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487efde0-4e7f-47b8-a4db-a49f48fb8751",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9eb435c6-c8ac-452e-bcee-d83d28eaa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ee264e8e-e261-4d71-8d14-feb0d671384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6094379917308969-2.6704972409568944e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b41cfa0b-39fa-49b7-a7f4-a2490212e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_2e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_2e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6911b-8624-48ad-9636-3059d040f72e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "de292ccd-10da-4435-b9fb-b10385d82eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 149.02 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "\n",
    "params['u'] = 1.3\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c38ac61e-3063-4628-a87f-c56e6e1f334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdc37a-7969-40b2-be98-111949d72836",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7f608320-d1bf-4cd6-ae97-2ea3d906c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fcf84222-06d8-4922-8f86-9b215e7c54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004280993120023111\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd86c8-12dd-4a95-ab8b-91ac649bd34d",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "64014bdc-725f-42a6-9e0d-225f82df744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e05bd-6b30-4cf2-8360-dbca224b9f71",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "32515660-0311-4b4b-8ab3-0e0604244f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac005b-82be-40fb-83a9-d444e81240db",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e42aee4f-88d1-475b-80a7-76705bc52939",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3e450e69-6a29-4b67-9768-be4ac542af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853cf100-147a-4e38-9f80-834563306dda",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4ade0ea7-93f4-465e-8b74-3de482b5cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36d1a4-20f1-41a8-8d90-a998799d0da6",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "769cb477-41c3-49f5-957b-12dd70d4222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "eb5f191d-22aa-44ac-abc0-a788665889f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3ce9c-c784-4076-b653-151d75961ad2",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a462565a-da8f-49e6-93dc-9d65278c8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "21798326-28e9-4a5e-a2c7-fcc2bd548106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5938832117125649-5.1387681117518025e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "77f16898-57e5-46d8-9816-9a7c6824ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_3e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_3e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71562ede-ab8b-464c-ba4e-3fc52119dc31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "62490d6e-20ad-4ee8-9ea8-31b7a28637b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 160.41 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.4\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3f4508cb-e68f-4aa2-a0a4-43c89923be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab02925-c50f-4279-bb9a-f5462054115f",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "96bf3c79-2a74-4ea9-9fe3-578bc0bd0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "40b573a2-02da-40ab-8031-64fdc04efb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004684071129829936\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dfbec-2d82-4423-a548-82ba5caec8c3",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d095ec11-7732-4a25-85bb-f42b18e25ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454be913-02cd-41a6-9885-430c1ffbee35",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "997ffdb4-97e3-4413-ae36-6abd40e67ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b51ea-1409-45e2-bf5b-753f26d2650f",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2d8465b8-3a69-45ab-ab01-eb19404ee457",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7b93eba6-de43-4037-b11c-f43d38c06e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8adac-bf00-4c36-befb-e9d83d699cdc",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fa1637e5-5366-4e65-b176-fef3dbbde84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944081d-46d3-4ebd-bffc-2a0ee94d9891",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "86120f75-e181-4825-9720-f9277d05eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7fc309fb-42f0-4dc9-a0db-aee0755fb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc21aaf-9eda-4e25-b001-e6c34969c324",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "261494b7-46da-4177-bb1f-ec7bccefbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "691bd180-2409-4d3d-ad67-66f88728f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5935187118152022+4.027739447190763e-20j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eca95802-e46e-4848-a80a-4ea7b916d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_4e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_4e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c55cb-f2c5-424d-a1e0-9a79034e1f3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7e109850-fed0-4179-95a2-eb31370a7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 158.66 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.5\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "cfd03e3d-8556-4a86-b05c-fe835598b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7b6d5-153b-4bb7-ba29-feafc4c6da09",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "06f2726a-bd0f-4cf8-9f73-36ebbe64f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f6b3ddbc-a1a5-4681-a110-bb7c08c5d624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00033894732325986574\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec88307-2e73-4f1b-af06-bfafa8f6981d",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fd18484c-a06d-4495-9a41-4c447da42ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e86335-c496-49cf-ad95-d1f0e3afe18e",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "cb7566e7-01c2-4821-942e-6ade657778a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff058be-0d54-4619-aa99-93d822ef4f9e",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "51ff98f7-8624-456e-bacb-c5f7c184f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "10e6e121-48de-492b-8da8-ff61777082b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab5526-bf2c-45bb-b228-63c6705e88a1",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "97ed6f27-c6b8-4f88-8475-2b60f28e4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c7e88-fad7-488f-8c3a-c936a6774bae",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "29f86276-f9d0-4e59-94a4-cc30307dbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8a5881f7-53b6-4ff5-9869-bc1b666c4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ce924-56ba-43d2-b54a-6ee2c7b92a76",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8d3a2a12-91de-40dc-a87b-b10e5285ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a01eec54-b953-4ae2-9433-696ce57e285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5836202271963725+7.941900741090982e-19j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2d7a11ae-b834-454b-9ac4-b6f126da986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_5e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_5e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee0807-56ed-456b-99a6-75fa24c228a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "531119f8-071f-438f-9028-e1407644b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 161.01 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "\n",
    "params['u'] = 1.6\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6d49abe8-8e17-4a05-acb9-6dbafd8e49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917383b-2779-4967-b680-5e933f1dfc3a",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "32b8241b-bd51-4544-a8b1-6869b8ae6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "108332ea-8a98-4d6d-9047-63f6777167bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002026018803580361\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921b1a3-4673-4652-9d98-8f4636eabbcf",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3f3a2924-a0d6-41fd-97cb-c9e07c580058",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f489f48-759e-4b47-8545-38c32e485fa4",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "cd407141-3d01-439d-bfea-95ef4c9d9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404c458-3016-4fb0-9648-cff8881ffb9e",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b3f91ec7-4326-4369-85d8-51b83f0aecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c7b40437-dcb9-41aa-b93e-411acf232cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971674b7-ee16-43d9-a5e6-088af109a9e2",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0538529c-e418-4ca2-a438-af5bb04983f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dafa84-26e6-4c41-a8ad-12d6eb29198b",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9977be5b-8d78-417b-a151-45971bf7292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a7fb8a54-f383-4b4a-a253-ddb392e4c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1cbe7-bd0e-49f0-bab8-7bb007c386c4",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5d7117fe-ce4c-468c-a4cb-165ce5146856",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8df120b9-9fd8-4cc3-89a5-57589579fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5701680934242517+9.941028196491671e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "39caa17a-1779-4d91-969a-9ec44fb1cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_6e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_6e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6168f8-2221-4048-9887-f5caffad7061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "04065528-10a7-4f63-9dc6-6e35a0c4a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 158.24 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "\n",
    "params['u'] = 1.7\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0d1dfc65-3dc1-4540-88b1-2e612c146efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9566e21-3281-4051-afe7-a6f65c1b76bc",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "60eddf03-9193-4716-ab54-eacefe1555be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fc6c8007-f685-4e3e-a479-72e3c1522fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00023298896301817055\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c37ae-3689-4d25-91a2-a515ea76fc2b",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2056e20b-409e-4456-ab00-9c66a51237ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f78c8-2852-48be-aedd-adcdfc9bfb4d",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6cf124e5-13e5-43db-b3e0-903a109d8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465d095-0c2c-4af3-8947-caa3e7c5eb56",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "863c5d84-b4b8-443b-82e7-be13917987ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "95dbdf5a-b0e6-4bd3-9de7-6cd32c53be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05eb612-a56b-4aad-8f3f-9c94c149f80e",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a9b960bc-fcea-43b5-bcf8-69eddb312fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd102e3-f4ef-4982-9826-706aea1e53f9",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c921ad1d-aee5-41b9-8198-03ed10a74da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5daf0e8c-cd35-42e7-8376-3c1d708b8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8893b0-f91d-4b5f-a29e-d8841c429f11",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0d9282cc-d3bc-4c95-912d-63e4b5962818",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9cc8abd4-ad40-4b14-afd3-804da461810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5641223016780018-1.1384149974646664e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5b0d898a-6948-453d-9ad4-e3ef61a1c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_7e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_7e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66763d61-29fb-4998-81c2-6b7dae8c7312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b34ffa63-4f81-4b56-bc75-0a22b2ae5454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 154.31 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.8\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "086be243-668f-4409-9681-2b985ed913f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042ad59-6265-4f2a-b769-0395fc6054d8",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "782d5aba-e945-4ddf-8ee8-3a79f7e7b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c4e00834-7570-47d3-858e-d19e6719a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00024372116306419043\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0b9be-8867-4619-a7d0-79a6bb22eaae",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ff09213a-df16-4ecf-ad09-bc008a6909bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370afddc-2a49-4fb7-9d2b-213df4bcc5c0",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6c49b797-6f43-489f-a701-5d16730e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9eb15b-1913-477f-937e-f108817c8f66",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3be09d4b-da6c-44ed-9ee3-56f1d254018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cafa329e-9950-40dd-8b81-0ae9ceebc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd50dcc-8f53-46cd-a8ed-b1a2b81d8d3c",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f28d61ef-671c-48c5-877e-59fa18ce04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd325f-b489-4e50-b664-c09f8ed08fc2",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "bdf74028-414d-41e6-82f2-086dae0b370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a95e9e01-109a-4aab-b5db-4b4b862cd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952feee6-0522-4261-a30d-66f588927def",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "033ea303-9cbb-43ca-885d-dff839166f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6349f66b-9a8b-4729-9f7d-a7577b953c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5583183556971124-1.6068710372597941e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7b330a46-5f7c-44be-8122-b38f8769b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_8e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_8e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e599f4-aa75-4499-87ff-0895d1e66cca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 1.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3c670c46-ea3d-400c-beab-6ddb49cdcae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 142.54 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 1.9\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cc9f6cca-6add-4001-b14c-7facdf40cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043dd88-3158-4a13-b294-b717ad02db0a",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3370c1f5-342b-44ea-9c92-33bfec2c9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d66c50b1-9ad0-465c-b8f0-a1c7b4b5c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002198966121783299\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d088a-c269-45db-88c1-b3c5cde7e62c",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4ea9f9eb-1225-4280-a2ea-390a60f32e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbd299-7a43-414f-8bf6-7084b6d7f11a",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "910115ae-f5a4-40b2-8476-27d37ae436ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d77ec7-7699-4e05-95e0-73288f795c8d",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cfb78a91-53bb-4f4c-837d-253ef2a667e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "93fa9902-b198-4661-8a31-556b2539d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c3861-d53b-48b1-a68a-df29723890f8",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "75ad3fab-e54a-4c04-b183-eb96961376f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4c137-63d4-4339-afa8-a768f1e5bc51",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "55b0665c-b989-4c57-8192-100875d6f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1a29add2-f895-420d-bb29-045eb75cb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ab304-1b40-4919-85dc-88f0f80f5f52",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "25992687-e1b3-4645-931a-1f0a0fc8891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c6d23b3b-c75f-410e-8cad-f7edffcd5bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5527393939814338+1.7765854273533978e-17j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "28c9ad6d-9d46-4b7f-9222-d80a131f90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U1_9e0_t1_0e0\n",
      "/std_deviation_numberOp_U1_9e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1087f15-0f41-436f-8833-4fb553e2a381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### $U = 2.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "61fff8c6-6329-4d59-ab8c-040827962a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 147.43 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#########################################\n",
    "\n",
    "params['u'] = 2\n",
    "driver.initialize_system(n_sites=params['L'], vacuum=Q(0), target=Q(params['N_BOSON']), hamil_init=False)\n",
    "driver.ghamil = driver.get_custom_hamiltonian(site_basis, site_ops)\n",
    "b = driver.expr_builder()\n",
    "\n",
    "# b.add_term(\"ADCB\", np.array([[i, i+1, i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"AD\", np.array([[i, i+1] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"CB\", np.array([[i+1, i] for i in range(params['L']-1)]).flatten(), -params['t'])\n",
    "b.add_term(\"N\", np.array(np.arange(params['L'])), -(params['mu'] + params['u'] / 2))\n",
    "b.add_term(\"NN\", np.repeat(np.arange(params['L']), 2), params['u'] / 2)\n",
    "\n",
    "mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "mps = driver.get_random_mps(tag=\"KET\", bond_dim=50, nroots=2)\n",
    "energy = driver.dmrg(mpo, mps, n_sweeps=10, bond_dims=[50] * 4 + [100] * 4 + [200] * 4 + [250] * 4 + [300] * 4 + [350] * 4 + [400] * 4 + [450] * 4 + [500] * 4 + [550] * 4 + [600] * 4,\n",
    "    noises=[1e-4] * 4 + [1e-5] * 4 + [0], thrds=[1e-10] * 8, dav_max_iter=30, iprint=0)\n",
    "\n",
    "#########################################\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d44b3a46-3ddb-4d97-ba20-f88b4281e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kets = [driver.split_mps(mps, ir, tag=\"KET-%d\" % ir) for ir in range(mps.nroots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36d575-47f9-4bbc-b968-0451d07dcd9b",
   "metadata": {},
   "source": [
    "##### First Excitation Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2688513d-9afd-4680-9901-83151ae861d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gndenergy = pd.DataFrame({'gndenergy':[np.power(np.abs(energy[1]-energy[0]),2)]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "## Complete path\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, gndstate_energy = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, gndenergy, filename1, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e079c21c-ddfd-467e-a0d9-3ea5a86ffb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012875350232110868\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "gndenergy = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(gndenergy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e8e82-724f-4bf5-a509-d4d28e74eaf7",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "30c8e5d8-3604-44a8-9d03-bafa75901800",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy1 = driver.get_bipartite_entanglement(kets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e438b38-e944-42c7-8be9-d2a17b2cbb68",
   "metadata": {},
   "source": [
    "###### HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9e5fd169-4083-4276-b371-088edc01f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(entropy1)\n",
    "filename = generate_file_name_hdf5(params, entropy = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, entropy = True)\n",
    "append_data_to_hdf5(params, entropy, filename, key = 'entropy', entropy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6f90c-c989-455d-88bb-b28763035d96",
   "metadata": {},
   "source": [
    "##### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3e7e029d-0ace-45c6-ae50-31e3a3684d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_exp_val = np.zeros((params['L'], params['L']), dtype=complex)\n",
    "for i in range(params['L']):\n",
    "    for j in range(i, params['L']):\n",
    "        b = driver.expr_builder()\n",
    "        b.add_term(\"CD\", np.array([i, j]), 1)  # Hopping operator between i and j            \n",
    "        hop_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "        hop_exp_val[i, j] = driver.expectation(kets[0], hop_mpo, kets[0])\n",
    "        hop_exp_val[j, i] = np.conjugate(hop_exp_val[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "74854106-a316-4650-9f0c-3840aefc16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix = pd.DataFrame(hop_exp_val)\n",
    "filename = generate_file_name_hdf5(params, Correlation = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, Correlation = True)\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)\n",
    "\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, correl_matrix, filename1, key = 'correl_matrix', Correlation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96f0dc-f4b8-48c6-ae6e-9e0c93b1aca6",
   "metadata": {},
   "source": [
    "##### NumberOperator and Its Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ac6773fe-30c3-4c89-a9e3-00bc3a078fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val, std_nOp = 0, 0\n",
    "_exp_nop = []\n",
    "for i in range(params['L']):\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"N\", np.array([i]), 1)\n",
    "## Calculate <N>   \n",
    "    nOp_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    impo = driver.get_identity_mpo()\n",
    "\n",
    "    nOp_exp = driver.expectation(kets[0], nOp_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0])\n",
    "    _exp_nop.append(nOp_exp)\n",
    "## Calculate <N^2>\n",
    "    b = driver.expr_builder()\n",
    "    b.add_term(\"NN\", np.repeat([i], 2), 1)\n",
    "    \n",
    "    NN_mpo = driver.get_mpo(b.finalize(adjust_order=True, fermionic_ops=\"\"), algo_type=MPOAlgorithmTypes.FastBipartite)\n",
    "    NN_exp = driver.expectation(kets[0], NN_mpo, kets[0]) / driver.expectation(kets[0], impo, kets[0]) \n",
    "## Calculate Standard Deviation\n",
    "    std_val+=np.sqrt(NN_exp - (nOp_exp*nOp_exp))        \n",
    "std_nOp = (std_val/params['L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60aadd2-29c1-4390-8998-e8a605641844",
   "metadata": {},
   "source": [
    "###### Expectation value of Number Operator on each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "34f8e85e-7caa-4741-93e9-9b5390e43bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_nop = pd.DataFrame(_exp_nop)\n",
    "filename = generate_file_name_hdf5(params, expnop = True)\n",
    "## Complete path of the location of the file\n",
    "filename1 = generating_complete_path_hdf5(filename, expnop = True)\n",
    "key = generate_unique_key(params, exp_nOp = True)\n",
    "append_data_to_hdf5(params, density_nop, filename, key = key, expnop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c8034846-b7e7-4116-b928-ce3e180a949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = generate_unique_key(params, exp_nOp = True)\n",
    "# density_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "# print(density_nop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e80d12-4ed5-4bba-98e4-0e0b379defd4",
   "metadata": {},
   "source": [
    "###### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "5b1eb22b-c4bf-4f8f-a8c2-c58d22d1d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation_numberOp = pd.DataFrame({'std_nOp':[std_nOp]})\n",
    "filename = generate_file_name_hdf5(params)\n",
    "filename1 = generating_complete_path_hdf5(filename)\n",
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "\n",
    "## Use the code below to append data to an existing file/folder directory\n",
    "append_data_to_hdf5(params, std_deviation_numberOp, filename1, key = key)\n",
    "\n",
    "####### Use this only for the first time when taking data for a theta #######\n",
    "## Use this code to write data to a new file irrespective of the presence of subholder based on 'L'.\n",
    "# create_folders_and_store_data(params, std_deviation_numberOp, filename, key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9e449520-5971-400f-8256-ff011c237234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.536188733530277-1.2909286344418562e-18j)\n"
     ]
    }
   ],
   "source": [
    "key = generate_unique_key(params, std_deviation_numberOp = True)\n",
    "std_nop = load_matrix_from_hdf5(filename1, key = key)\n",
    "print(std_nop[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "57c7146c-926d-47a0-80a0-5cbdd47fa1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file:\n",
      "/gndenergy_U2_0e0_t1_0e0\n",
      "/std_deviation_numberOp_U2_0e0_t1_0e0\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(filename1, 'r') as hdf:\n",
    "    # List all keys in the file\n",
    "    keys = hdf.keys()\n",
    "    print(\"Keys in HDF5 file:\")\n",
    "    for key in keys:\n",
    "        print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
